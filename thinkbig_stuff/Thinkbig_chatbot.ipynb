{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3zBvBern-CT",
        "outputId": "602b3075-f5e4-43ff-9a6f-c1e83805fda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  8 00:21:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPU 정보 #런타임 유형 GPU로 해야 됩니다.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소스 코드 복사\n",
        "!git clone https://github.com/InhwanCho/chatbot_think_big.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9XfNixqoDDA",
        "outputId": "24a26629-988b-4c91-9d62-bfe430ca5520"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot_think_big'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 33 (delta 16), reused 25 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd chatbot_think_big"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQ1B3PxoRiX",
        "outputId": "59b88932-8805-49d6-be24-d3f85aab3070"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chatbot_think_big\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DyDeJpkloWbh",
        "outputId": "6f80a64c-d083-4c57-a91a-4a6b6ffff345"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/chatbot_think_big'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "kekm53H_oW_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda91c51-0862-41e2-96b8-b3e3a68031e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.13.0+cu116)\n",
            "Collecting torchtext==0.10.1\n",
            "  Downloading torchtext-0.10.1-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 19.8 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning==1.2.10\n",
            "  Downloading pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
            "\u001b[K     |████████████████████████████████| 841 kB 94.9 MB/s \n",
            "\u001b[?25hCollecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.1->-r requirements.txt (line 2)) (4.64.1)\n",
            "Collecting torch\n",
            "  Downloading torch-1.9.1-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 9.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (4.4.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 100.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (2.9.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (2022.11.0)\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 91.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 74.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.8.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.51.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (2.15.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.1->-r requirements.txt (line 2)) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch_lightning==1.2.10->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (1.2.0)\n",
            "Building wheels for collected packages: future, sacremoses\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491071 sha256=2f10915d8b48fcd4c610e294a7f98743edd19f5420445c166f417def03591a9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=13dbe1c8ed4c9553445a224db23100160bf1c2d901825284c706bc5928f0cda8\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built future sacremoses\n",
            "Installing collected packages: torch, torchmetrics, tokenizers, sacremoses, future, transformers, torchtext, pytorch-lightning\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.0\n",
            "    Uninstalling torchtext-0.14.0:\n",
            "      Successfully uninstalled torchtext-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed future-0.18.2 pytorch-lightning-1.2.10 sacremoses-0.0.53 tokenizers-0.10.3 torch-1.9.1 torchmetrics-0.2.0 torchtext-0.10.1 transformers-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 최대 사용 용량 올리기 (이거 해야 경고메시지 안나옴 코랩에서는 안됨)\n",
        "!export TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=107374182400"
      ],
      "metadata": {
        "id": "7LVsG1OJupBf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "ZSSENf0B5gie"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
        "!CUDA_VISIBLE_DEVICES=0 python thinkbigchatbot.py --train --gpus 0 --max_epochs 1"
      ],
      "metadata": {
        "id": "898j-owWobGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db7d5bb-3c7f-46ac-bc1c-6d2f07e89784"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-08 00:22:48.926059: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading: 100% 2.83M/2.83M [00:01<00:00, 1.77MB/s]\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=128, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=0, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=1, max_len=64, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=True, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "Downloading: 100% 1.00k/1.00k [00:00<00:00, 958kB/s]\n",
            "Downloading: 100% 513M/513M [00:05<00:00, 86.3MB/s]\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 125 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "---------------------------------------------------\n",
            "125 M     Trainable params\n",
            "0         Non-trainable params\n",
            "125 M     Total params\n",
            "500.656   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 0:   0% 0/452 [00:00<?, ?it/s] INFO:root:contexts : 질긴 인연 드디어 끝인 것 같네\n",
            "INFO:root:toked ctx: ['<usr>', '▁질', '긴', '▁인', '연', '▁드디어', '▁끝', '인', '▁것', '▁같', '네', '<unused1>', '▁1']\n",
            "INFO:root:response : 후련하길 바랍니다.\n",
            "INFO:root:contexts : 친구 생일 선물로 시계를 샀어. 기분이 너무 좋네.\n",
            "INFO:root:toked response : ['<sys>', '▁후', '련', '하', '길', '▁바', '랍', '니다.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁후', '련', '하', '길', '▁바', '랍', '니다.', '</s>']\n",
            "INFO:root:toked ctx: ['<usr>', '▁친구', '▁생일', '▁선', '물로', '▁시', '계를', '▁샀', '어', '.', '▁기', '분이', '▁너무', '▁좋', '네.', '<unused1>', '▁2']\n",
            "INFO:root:response : 친구 생일 선물을 사셨군요. 어떤 부분에서 기분이 가장 좋으신가요?\n",
            "INFO:root:toked response : ['<sys>', '▁친구', '▁생일', '▁선물을', '▁사', '셨', '군', '요.', '▁어떤', '▁부분에서', '▁기', '분이', '▁가장', '▁좋', '으', '신', '가', '요', '?', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁친구', '▁생일', '▁선물을', '▁사', '셨', '군', '요.', '▁어떤', '▁부분에서', '▁기', '분이', '▁가장', '▁좋', '으', '신', '가', '요', '?', '</s>']\n",
            "tcmalloc: large alloc 1677721600 bytes == 0x7f3496800000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c5464469 0x7f36c54651b7 0x7f36c5841749 0x7f36c5fa655a 0x7f36c5f8967e 0x7f36c5b8e3c5 0x7f36c56b453f 0x7f36c5fa76df 0x7f36c5dd1989 0x7f36c5dd427b 0x7f36c76c60fa 0x7f36c76c6852 0x7f36c6277185 0x7f36c56b938d 0x7f36c56b9cca 0x7f36c611694f 0x7f36c5aa8985 0x7f36c569a0f3 0x7f36c6116632 0x7f36c5bb204f 0x7f36d85f5c0e 0x5d746e 0x5d813c 0x56376d 0x55e571 0x5d7cf1 0x49caa1\n",
            "tcmalloc: large alloc 3355443200 bytes == 0x7f33c98f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "tcmalloc: large alloc 1677721600 bytes == 0x7f334c8f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c584b53b 0x7f36c5fa65af 0x7f36c5f8ae40 0x7f36c5b93e14 0x7f36c584b9db 0x7f36c6115cd4 0x7f36c5b9182e 0x7f36c5846a75 0x7f36c61170e4 0x7f36c5b91c76 0x7f36d8426923 0x5d746e 0x5d813c 0x56376d 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051 0x5d7c18 0x49ca7c 0x5d7c18\n",
            "tcmalloc: large alloc 1677721600 bytes == 0x7f32e88f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c5464469 0x7f36c54651b7 0x7f36c5841749 0x7f36c5fe2726 0x7f36c544f11b 0x7f36c5450553 0x7f36c5450ebd 0x7f36c5ff617d 0x7f36c5ff620f 0x7f36c5dd1989 0x7f36c5dd47fb 0x7f36c7762955 0x7f36c77633a2 0x7f36c6277455 0x7f36d8306170 0x7f36d8306a16 0x4f99fb 0x46eb22 0x4e841c 0x5dcf0b 0x658395 0x5173b2 0x55fe85 0x5d7c18 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   0% 1/452 [00:24<3:06:51, 24.86s/it, loss=43.5, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32f60f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   0% 2/452 [00:48<3:01:32, 24.21s/it, loss=43.6, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32920f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   1% 3/452 [01:12<3:00:31, 24.12s/it, loss=42.8, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32920f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   1% 4/452 [01:35<2:58:21, 23.89s/it, loss=42.6, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32920f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   1% 5/452 [01:58<2:57:09, 23.78s/it, loss=41.9, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32920f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0:   1% 6/452 [02:22<2:56:05, 23.69s/it, loss=41.4, v_num=0]tcmalloc: large alloc 3355443200 bytes == 0x7f32920f8000 @  0x7f370349ab6b 0x7f37034ba379 0x7f3681bf326e 0x7f3681bf49e2 0x7f36c6315433 0x7f36c57da45d 0x7f36c57db040 0x7f36c58310ab 0x7f36c5833bf0 0x7f36c5fa8d12 0x7f36c5e2b0d4 0x7f36c75b8df1 0x7f36c75b9245 0x7f36c62f23e5 0x7f36c57c3af5 0x7f36c57c4829 0x7f36c6117829 0x7f36c62d0852 0x7f36d82e22db 0x5d746e 0x5d813c 0x560200 0x5d7c18 0x4fea58 0x5d78b6 0x561051 0x55e571 0x5d7cf1 0x4fea58 0x5d78b6 0x561051\n",
            "Epoch 0: 100% 452/452 [2:58:34<00:00, 23.70s/it, loss=38.5, v_num=0]Epoch 0, global step 451: train_loss reached 37.76346 (best 37.76346), saving model to \"/content/chatbot_think_big/model_chp/model_-epoch=00-train_loss=37.76.ckpt\" as top 1\n",
            "Saving latest checkpoint...\n",
            "Epoch 0: 100% 452/452 [2:58:44<00:00, 23.73s/it, loss=38.5, v_num=0]\n",
            "INFO:root:best model path /content/chatbot_think_big/model_chp/model_-epoch=00-train_loss=37.76.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화 테스트, `quit`를 입력하면 대화를 종료합니다.\n",
        "!CUDA_VISIBLE_DEVICES=0 python thinkbigchatbot.py --gpus 1 --chat"
      ],
      "metadata": {
        "id": "E1CUSxRsoiAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d49869-dba2-47a5-8da2-4bf4fd9ee20c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-08 03:53:59.755488: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=128, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=None, max_len=64, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=False, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "user > 안녕 오늘은 신나는 목요일이네.\n",
            "Think_big > 오늘은 목요일이네요.\n",
            "user > 내일도 신나는 금요일이야.\n",
            "Think_big > 금요일이죠.\n",
            "user > 너 대화 좀 할 줄 아는구나.\n",
            "Think_big > 대화를 많이 하세요.\n",
            "user > 나 맥이지 말고 오늘 힘든 일 있는지 없는지 맞춰볼래??\n",
            "Think_big > 오늘 힘든 일이 있으셨나봐요.\n",
            "user > 너 챗봇 이 파일이 내 로컬에서 오류가 난다.\n",
            "Think_big > 로컬에서 오류가 난다니 많이 당황스러우시겠어요.\n",
            "user > 그냥 멘탈 관리의 필요가 느껴져..\n",
            "Think_big > 스트레스 받지 마세요.\n",
            "user > 그래 고맙다 오늘 하루도 수고하고.\n",
            "Think_big > 오늘 하루도 수고하셨군요.\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Y7yuuKBYoHI"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}